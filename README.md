<img src="./tmpimg/LOGO.gif" height="200" width="400"/>


# FightingCV-Paper-Reading


Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€

ä½œä¸ºç ”ç©¶ç”Ÿï¼Œè¯»è®ºæ–‡ä¸€ç›´éƒ½æ˜¯éƒ½æ˜¯ä¸€ä»¶éå¸¸**è´¹æ—¶è´¹è„‘**çš„äº‹æƒ…ï¼Œå› ä¸ºå¸®åŠ©å¤§å®¶ç”¨**5åˆ†é’Ÿ**çš„æ—¶é—´å°±èƒ½çŸ¥é“æŸç¯‡è®ºæ–‡çš„å¤§è‡´å†…å®¹ï¼Œæˆ‘ä¼šæŠŠæˆ‘çœ‹è¿‡çš„è®ºæ–‡åšå¥½è§£æåˆ†äº«åœ¨è¿™é‡Œã€‚**é¡¹ç›®æŒç»­æ›´æ–°ï¼Œæ¯å‘¨è‡³å°‘æ›´æ–°ä¸‰ç¯‡ï¼**â­â­â­

æœ¬é¡¹ç›®çš„å®—æ—¨æ˜¯ğŸš€**è®©ä¸–ç•Œä¸Šæ²¡æœ‰éš¾è¯»çš„è®ºæ–‡**ğŸš€ï¼Œè®ºæ–‡ä¸»é¢˜åŒ…æ‹¬ä½†ä¸é™äºæ£€æµ‹ã€åˆ†ç±»ã€åˆ†å‰²ã€Backboneã€å¤šæ¨¡æ€ç­‰ç­‰ï¼Œè®ºæ–‡æ¥æºåŒ…æ‹¬ä½†ä¸é™äºæœ€æ–°çš„arXivè®ºæ–‡ã€ICCV2021ã€CVPR2021ã€MM2021ã€‚**(é¡¹ç›®ä¼šä¿æŒæŒç»­æ›´æ–°ï¼Œæ¯å‘¨è‡³å°‘ä¸‰ç¯‡)**â­â­â­
 
 
ï¼ˆæœ€æ–°è¿˜æ›´æ–°äº†[ã€Attentionã€MLPã€Convã€MLPã€Backboneçš„ä»£ç å¤ç°é¡¹ç›®ã€‘](https://github.com/xmu-xiaoma666/External-Attention-pytorch)ï¼Œæ¬¢è¿å¤§å®¶å­¦ä¹ äº¤æµï¼‰


***
## å…¬ä¼—å· & å¾®ä¿¡äº¤æµç¾¤

æ¬¢è¿å¤§å®¶å…³æ³¨å…¬ä¼—å·ï¼š**FightingCV**

å…¬ä¼—å·**æ¯å¤©**éƒ½ä¼šè¿›è¡Œ**è®ºæ–‡ã€ç®—æ³•å’Œä»£ç çš„å¹²è´§åˆ†äº«**å“¦~


å·²å»ºç«‹**æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç®—æ³•/è®¡ç®—æœºè§†è§‰/å¤šæ¨¡æ€äº¤æµç¾¤**å¾®ä¿¡äº¤æµç¾¤ï¼

ï¼ˆåŠ ä¸è¿›å»å¯ä»¥åŠ å¾®ä¿¡ï¼š**775629340**ï¼Œè®°å¾—å¤‡æ³¨ã€**å…¬å¸/å­¦æ ¡+æ–¹å‘+ID**ã€‘ï¼‰

**æ¯å¤©åœ¨ç¾¤é‡Œåˆ†äº«ä¸€äº›è¿‘æœŸçš„è®ºæ–‡å’Œè§£æ**ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·**å­¦ä¹ äº¤æµ**å“ˆ~~~

![](./tmpimg/wechat.jpg)

å¼ºçƒˆæ¨èå¤§å®¶å…³æ³¨[**çŸ¥ä¹**](https://www.zhihu.com/people/jason-14-58-38/posts)è´¦å·å’Œ[**FightingCVå…¬ä¼—å·**](https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w)ï¼Œå¯ä»¥å¿«é€Ÿäº†è§£åˆ°æœ€æ–°ä¼˜è´¨çš„å¹²è´§èµ„æºã€‚

***


## æ€»ç»“æ€§æ–‡ç« 

- [ä»å¤šç¯‡2021å¹´é¡¶ä¼šè®ºæ–‡çœ‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹æœ€æ–°ç ”ç©¶è¿›å±•](https://zhuanlan.zhihu.com/p/425859974)  


- [æ·±åº¦å­¦ä¹ ä¸­çš„é‡å‚æ•°æœºåˆ¶æ€»ç»“ä¸ä»£ç å®ç°](https://zhuanlan.zhihu.com/p/383660483)  

- [æ·±åº¦å­¦ä¹ ä¸­çš„Attentionæ€»ç»“ï¼ˆä¸€ï¼‰](https://zhuanlan.zhihu.com/p/379657870)  

- [æ·±åº¦å­¦ä¹ ä¸­çš„Attentionæ€»ç»“ï¼ˆäºŒï¼‰](https://zhuanlan.zhihu.com/p/386333201)  

- [æ€è€ƒNLPå’ŒCVä¸­çš„Localå’ŒGlobalå»ºæ¨¡](https://zhuanlan.zhihu.com/p/387766129)  

## NeurIPS2021
### Transformer


- [NeurIPS2021-ã€ŠHRFormerã€‹-HRNetåˆå‡ºç»­ä½œå•¦ï¼å›½ç§‘å¤§&åŒ—å¤§&MSRAæå‡ºé«˜åˆ†è¾¨ç‡Transformerï¼Œä»£ç å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/429936715)  
    [ã€HRFormer: High-Resolution Transformer for Dense Predictionã€‘](https://arxiv.org/abs/2110.09408)

- [NeurIPS2021-ViTç°åœ¨å¯ä»¥åšç›®æ ‡æ£€æµ‹ä»»åŠ¡å•¦ï¼åç§‘æå‡ºç›®æ ‡æ£€æµ‹æ–°æ–¹æ³•YOLOS](https://zhuanlan.zhihu.com/p/4262947628)  
    [ã€You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detectionã€‘](https://www.arxiv-vanity.com/papers/2106.00666/)

- [NeurIPS2021-æ²¡æœ‰æ®‹å·®è¿æ¥çš„ViTå‡†ç¡®ç‡åªæœ‰0.15%ï¼ï¼ï¼åŒ—å¤§&åä¸ºæå‡ºç”¨äºVision Transformerçš„Augmented Shortcutsï¼Œæ¶¨ç‚¹æ˜¾è‘—ï¼](https://zhuanlan.zhihu.com/p/424214038)  
    [ã€Augmented Shortcuts for Vision Transformersã€‘](https://arxiv.org/abs/2106.15941)

- [NeurIPS2021- Transformeréƒ¨ç½²éš¾ï¼ŸåŒ—å¤§&åä¸ºè¯ºäºšæå‡ºVision Transformerçš„åè®­ç»ƒé‡åŒ–æ–¹æ³•](https://zhuanlan.zhihu.com/p/423936004)  
    [ã€Post-Training Quantization for Vision Transformerã€‘](https://arxiv.org/abs/2106.14156)

- [Multi-Scale Densenetç»­ä½œï¼ŸåŠ¨æ€ViT](https://zhuanlan.zhihu.com/p/386929227)  
    [ã€Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Lengthã€‘](https://arxiv.org/abs/2105.15075)

- [å¾®è½¯æ–°ä½œFocal Self-Attentionï¼šå…·å¤‡Localå’ŒGlobaläº¤äº’èƒ½åŠ›çš„Transformer](https://zhuanlan.zhihu.com/p/387693270)  
    [ã€Focal Self-attention for Local-Global Interactions in Vision Transformersã€‘](https://arxiv.org/abs/2107.00641)


***

### å¤šæ¨¡æ€

- [NeurIPS2021-ã€ŠMBTã€‹-å¤šæ¨¡æ€æ•°æ®æ€ä¹ˆèåˆï¼Ÿè°·æ­Œæå‡ºåŸºäºæ³¨æ„åŠ›ç“¶é¢ˆçš„æ–¹æ³•ï¼Œç®€å•é«˜æ•ˆè¿˜çœè®¡ç®—é‡](https://zhuanlan.zhihu.com/p/427779731)  
    [ã€Attention Bottlenecks for Multimodal Fusionã€‘](https://arxiv.org/abs/2107.00135)


- [NeurIPS2021-å¿«æ¥åˆ·æ¦œå§ï¼å¾®è½¯æå‡ºæ–°çš„è§†é¢‘å¤šæ¨¡æ€benchmarkï¼ŒåŒæ—¶åŒ…å«æ£€ç´¢ã€captionã€QAç­‰å¤šä¸ªä»»åŠ¡ï¼](https://zhuanlan.zhihu.com/p/433827807)  
    [ã€VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluationã€‘](https://arxiv.org/abs/2106.04632)

***


### åŠ¨æ€ç½‘ç»œ

- [NeurIPS2021-ç”¨å¤šå¤§åˆ†è¾¨ç‡çš„å›¾åƒåšåˆ†ç±»æ›´é€‚åˆï¼Ÿæµ™å¤§&åä¸º&å›½ç§‘å¤§æå‡ºDynamic Resolution Networkï¼Œé™ä½è®¡ç®—é‡è¿˜èƒ½ææ€§èƒ½ï¼](https://zhuanlan.zhihu.com/p/428436758)  
    [ã€Dynamic Resolution Networkã€‘](https://arxiv.org/abs/2106.02898)


***


## ICCV2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [ICCV2021 Oral-MDETRï¼šå›¾çµå¥–å¾—ä¸»Yann LeCunçš„å›¢é˜Ÿ&Facebookæå‡ºç«¯åˆ°ç«¯å¤šæ¨¡æ€ç†è§£çš„ç›®æ ‡æ£€æµ‹å™¨](https://zhuanlan.zhihu.com/p/394239659)  
    [ã€MDETR -- Modulated Detection for End-to-End Multi-Modal Understandingã€‘](https://arxiv.org/abs/2104.12763)

- [ICCV2021-NTUç”¨å¤šæ ·æ€§çš„queryç”Ÿæˆï¼Œæ¶¨ç‚¹åŸºäºæ–‡æœ¬çš„å®ä¾‹åˆ†å‰²ï¼ˆå·²å¼€æºï¼‰](https://zhuanlan.zhihu.com/p/404955179)  
    [ã€Vision-Language Transformer and Query Generation for Referring Segmentationã€‘](https://arxiv.org/abs/2108.05565)

- [ICCV2021-å¦‚ä½•é«˜æ•ˆè§†é¢‘å®šä½ï¼ŸåŒ—å¤§&Adobe&QMULå¼ºå¼ºè”æ‰‹æå‡ºå¼±ç›‘ç£CRMï¼Œæ€§èƒ½SOTA](https://zhuanlan.zhihu.com/p/406704588)  
    [ã€Cross-Sentence Temporal and Semantic Relations in Video Activity Localisationã€‘](https://arxiv.org/abs/2107.11443)
    
- [ICCV2021-TOCo-å¾®è½¯&CMUæå‡ºTokenæ„ŸçŸ¥çš„çº§è”å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåœ¨è§†é¢‘æ–‡æœ¬å¯¹é½ä»»åŠ¡ä¸Šâ€œåŠæ‰“â€å…¶ä»–SOTAæ–¹æ³•](https://zhuanlan.zhihu.com/p/406827017)  
    [ã€TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignmentã€‘](https://arxiv.org/abs/2108.09980)

- [ICCV2021 Oral-æ–°ä»»åŠ¡ï¼æ–°æ•°æ®é›†ï¼åº·å¥ˆå°”å¤§å­¦æå‡ºäº†ç±»ä¼¼VGä½†åˆä¸æ˜¯VGçš„PVGä»»åŠ¡](https://zhuanlan.zhihu.com/p/407102211)  
    [ã€Whoâ€™s Waldo? Linking People Across Text and Imagesã€‘](https://arxiv.org/abs/2108.07253)


- [ICCV2021-æ–°ä»»åŠ¡ï¼NTU&æ¸¯ä¸­æ–‡æå‡ºä»¥å¯¹è¯çš„æ–¹å¼è¿›è¡Œç»†ç²’åº¦çš„å›¾ç‰‡ç¼–è¾‘](https://zhuanlan.zhihu.com/p/418089405) 
    [ã€Talk-to-Edit: Fine-Grained Facial Editing via Dialogã€‘](https://arxiv.org/abs/2109.04425)



- [ICCV2021-ç”¨DETRçš„æ–¹æ³•åšDense Video Captioningï¼æ¸¯å¤§&å—ç§‘å¤§æå‡ºç«¯åˆ°ç«¯PDVCï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ã€‚](https://zhuanlan.zhihu.com/p/418100751) 
    [ã€End-to-End Dense Video Captioning with Parallel Decodingã€‘](https://arxiv.org/abs/2108.07781)


### å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
- [ICCV2021-DetCoï¼šæ€§èƒ½ä¼˜äºä½•æºæ˜ç­‰äººæå‡ºçš„MoCo v2ï¼Œä¸ºç›®æ ‡æ£€æµ‹å®šåˆ¶ä»»åŠ¡çš„å¯¹æ¯”å­¦ä¹ ã€‚](https://zhuanlan.zhihu.com/p/393202411)  
    [ã€DetCo: Unsupervised Contrastive Learning for Object Detectionã€‘](https://arxiv.org/abs/2102.04803)


### å¯è§£é‡Šæ€§ï¼ˆInterpretabilityï¼‰
- [ICCV2021 Oral-TAU&Facebookæå‡ºäº†é€šç”¨çš„Attentionæ¨¡å‹å¯è§£é‡Šæ€§](https://zhuanlan.zhihu.com/p/394794493)  
    [ã€Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformersã€‘](https://arxiv.org/abs/2103.15679)

- [ICCV2021 -ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿåˆ†ç±»æ­£ç¡®ï¼ŸSCOUTERèƒ½å¤Ÿâ€œæ­£â€â€œåâ€ä¸¤ä¸ªæ–¹é¢è¯´æœä½ ã€‚](https://zhuanlan.zhihu.com/p/396783525)  
    [ã€SCOUTER: Slot Attention-based Classifier for Explainable Image Recognitionã€‘](https://arxiv.org/abs/2009.06138)


### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [ICCV2021-iRPE-è¿˜åœ¨é­”æ”¹Transformerç»“æ„å—ï¼Ÿå¾®è½¯&ä¸­å±±å¤§å­¦æå‡ºè¶…å¼ºçš„å›¾ç‰‡ä½ç½®ç¼–ç ï¼Œæ¶¨ç‚¹æ˜¾è‘—](https://zhuanlan.zhihu.com/p/395766591)   
    [ã€Rethinking and Improving Relative Position Encoding for Vision Transformerã€‘](https://arxiv.org/abs/2107.14222)

- [ICCV2021 | æ± åŒ–æ“ä½œä¸æ˜¯CNNçš„ä¸“å±ï¼ŒVision Transformerè¯´ï¼šâ€œæˆ‘ä¹Ÿå¯ä»¥â€ï¼›å—å¤§æå‡ºæ± åŒ–è§†è§‰Transformerï¼ˆPiTï¼‰](https://zhuanlan.zhihu.com/p/398763751)  
    [ã€Rethinking Spatial Dimensions of Vision Transformersã€‘](https://arxiv.org/abs/2103.16302)

- [ICCV2021 | CNN+Transformer=Betterï¼Œå›½ç§‘å¤§&åä¸º&é¹åŸå®éªŒå®¤ å‡ºConformerï¼Œ84.1% Top-1å‡†ç¡®ç‡](https://zhuanlan.zhihu.com/p/400244375)  
    [ã€Conformer: Local Features Coupling Global Representations for Visual Recognitionã€‘](https://arxiv.org/abs/2105.03889)

- [ICCV2021 | MicroNets-æ›´å°æ›´å¿«æ›´å¥½çš„MicroNetï¼Œä¸‰å¤§CVä»»åŠ¡éƒ½ç§’æ€MobileNetV3](https://zhuanlan.zhihu.com/p/400661708)  
    [ã€MicroNet: Improving Image Recognition with Extremely Low FLOPsã€‘](https://arxiv.org/abs/2108.05894)


- [ICCV2021-MIT-IBM AI Labå¼€æºCrossViTï¼ŒTransformerå¼€å§‹èµ°å‘å¤šåˆ†æ”¯ã€å¤šå°ºåº¦ï¼ˆé™„ç›®å‰å¤šå°ºåº¦ViTçš„å¼‚åŒç‚¹å¯¹æ¯”ï¼‰](https://zhuanlan.zhihu.com/p/418086070)  
    [ã€CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classificationã€‘](https://arxiv.org/abs/2103.14899)



### å¤šä»»åŠ¡ï¼ˆMulti-Taskï¼‰
- [ICCV2021-MuST-è¿˜åœ¨ç‰¹å®šä»»åŠ¡é‡Œä¸ºåˆ·ç‚¹è€Œè‹¦è‹¦æŒ£æ‰ï¼Ÿè°·æ­Œçš„å¤§ä½¬ä»¬éƒ½å·²ç»å¼€å§‹ç©å¤šä»»åŠ¡è®­ç»ƒäº†](https://zhuanlan.zhihu.com/p/406014791)  
    [ã€Multi-Task Self-Training for Learning General Representationsã€‘](https://arxiv.org/abs/2108.11353)

- [ICCV2021-CVå¤šä»»åŠ¡æ–°è¿›å±•ï¼ä¸€èŠ‚æ›´æ¯”ä¸‰èŠ‚å¼ºçš„MultiTask CenterNetï¼Œç”¨ä¸€ä¸ªç½‘ç»œåŒæ—¶å®Œæˆç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œäººä½“å§¿æ€ä¼°è®¡ä¸‰ä¸ªä»»åŠ¡](https://zhuanlan.zhihu.com/p/405652732)  
    [ã€MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approachã€‘](https://arxiv.org/abs/2108.05060)

### æ•°æ®å¢å¼º

- [ICCV 2021ï½œâ€œç™½å«–â€æ€§èƒ½çš„MixMoï¼Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºoræ¨¡å‹èåˆæ–¹æ³•](https://zhuanlan.zhihu.com/p/418098973)   
    [ã€MicroNet: Improving Image Recognition with Extremely Low FLOPsã€‘](https://arxiv.org/abs/2108.05894)


- [ICCV2021 Oral-ç®€å•é«˜æ•ˆçš„æ•°æ®å¢å¼ºï¼åä¸ºæå‡ºäº†ä¸€ç§ç®€å•çš„é²æ£’ç›®æ ‡æ£€æµ‹è‡ªé€‚åº”æ–¹æ³•](https://zhuanlan.zhihu.com/p/396528978)   
    [ã€SimROD: A Simple Adaptation Method for Robust Object Detectionã€‘](https://arxiv.org/abs/2107.13389)


### å…¶ä»–


- [ICCV'21 Oralï½œæ‹’ç»è°ƒå‚ï¼Œæ˜¾è‘—æç‚¹ï¼æ£€æµ‹åˆ†å‰²ä»»åŠ¡çš„æ–°æŸå¤±å‡½æ•°RS Losså¼€æº](https://zhuanlan.zhihu.com/p/397519850)  
    [ã€Rank & Sort Loss for Object Detection and Instance Segmentationã€‘](https://arxiv.org/abs/2107.11669)

- [ICCV21 | å¤§é“è‡³ç®€ï¼Œä»…éœ€4è¡Œä»£ç æå‡å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ï¼ å—å¤§æå‡ºResidual Attention](https://zhuanlan.zhihu.com/p/397990353)  
    [ã€Residual Attention: A Simple but Effective Method for Multi-Label Recognitionã€‘](https://arxiv.org/abs/2108.02456)

- [ICCV2021 Oral-UNO-ç”¨äºNovel Class Discovery çš„ç»Ÿä¸€ç›®æ ‡å‡½æ•°ï¼Œç®€åŒ–è®­ç»ƒæµç¨‹ï¼å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/407365987)  
    [ã€A Unified Objective for Novel Class Discoveryã€‘](https://arxiv.org/abs/2108.08536)

- [ICCV2021-åˆ«é­”æ”¹ç½‘ç»œäº†ï¼Œæ¨¡å‹ç²¾åº¦ä¸é«˜ï¼Œæ˜¯ä½ Resizeçš„æ–¹æ³•ä¸å¤Ÿå¥½ï¼Googleæå‡ºåŸºäºDLçš„è°ƒæ•´å™¨æ¨¡å‹å­¦ä¹ æ›´å¥½çš„Resizeæ–¹æ³•](https://zhuanlan.zhihu.com/p/409582813)  
    [ã€Learning to Resize Images for Computer Vision Tasksã€‘](https://arxiv.org/abs/2103.09950)

- [ICCV2021-ã€ŠGroupFormerã€‹-å•†æ±¤&æ¸¯ç†å·¥æå‡ºåŸºäºèšç±»çš„è”åˆå»ºæ¨¡æ—¶ç©ºå…³ç³»çš„GroupFormerç”¨äºè§£å†³ç¾¤ä½“æ´»åŠ¨è¯†åˆ«é—®é¢˜ï¼Œæ€§èƒ½SOTA](https://zhuanlan.zhihu.com/p/411674711)  
    [ã€GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformerã€‘](https://arxiv.org/abs/2108.12630)


- [ICCV2021-å»é™¤å†—ä½™tokençš„DETRæ•ˆæœæ€ä¹ˆæ ·ï¼ŸNUSé¢œæ°´æˆå¤§ä½¬å›¢é˜Ÿç»™å‡ºäº†ç­”æ¡ˆï¼](https://zhuanlan.zhihu.com/p/415579801)  
    [ã€PnP-DETR: Towards Efficient Visual Analysis with Transformersã€‘](https://arxiv.org/abs/2109.07036)



- [ICCV2021-è¿˜åœ¨ç”¨å¤§é‡æ•°æ®æš´åŠ›trainæ¨¡å‹ï¼Ÿä¸»åŠ¨å­¦ä¹ ï¼Œæ•™ä½ é€‰å‡ºæ•°æ®é›†ä¸­æœ€æœ‰ä»·å€¼çš„æ ·æœ¬](https://zhuanlan.zhihu.com/p/420756941)  
    [ã€Active Learning for Deep Object Detection via Probabilistic Modelingã€‘](https://arxiv.org/abs/2103.16130)




- [ICCV2021-æ¯”MoCoæ›´é€šç”¨çš„å¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œä¸­ç§‘å¤§&MSRAæå‡ºå¯¹æ¯”å­¦ä¹ æ–°æ–¹æ³•MaskCo](https://zhuanlan.zhihu.com/p/4209392131)  
    [ã€Self-Supervised Visual Representations Learning by Contrastive Mask Predictionã€‘](https://arxiv.org/abs/2108.07954)


***

## ACM MM2021
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [ACM MM2021-è¿˜åœ¨ç”¨ViTçš„16x16 Patchåˆ†å‰²æ–¹æ³•å—ï¼Ÿä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€æå‡ºDeformable Patch-basedæ–¹æ³•ï¼Œæ¶¨ç‚¹æ˜¾è‘—ï¼](https://zhuanlan.zhihu.com/p/399417704)  
    [ã€DPT: Deformable Patch-based Transformer for Visual Recognitionã€‘](https://arxiv.org/abs/2107.14467)

- [ACMMM 2021-å¤šæ¨¡æ€å®è—ï¼äº¬ä¸œæ¢…æ¶›å›¢é˜Ÿé‡ç£…å¼€æºç¬¬ä¸€ä¸ªé€‚ç”¨äºå¤šä¸ªä»»åŠ¡çš„å¤šæ¨¡æ€ä»£ç åº“x-modalerï¼](https://zhuanlan.zhihu.com/p/403688076)  
    [ã€X-modaler: A Versatile and High-performance Codebase for Cross-modal Analyticsã€‘](https://arxiv.org/abs/2108.08217)

- [ACMMM 2021-æ€§èƒ½SOTAï¼ç”¨GNNå’ŒGANçš„æ–¹å¼æ¥å¼ºåŒ–Video Captioningçš„å­¦ä¹ ï¼](https://zhuanlan.zhihu.com/p/403895573)  
    [ã€Discriminative Latent Semantic Graph for Video Captioningã€‘](https://arxiv.org/abs/2108.03662)

***

## ICML2021
### é¢„è®­ç»ƒï¼ˆpre-trainï¼‰
- [ICML2021-ã€ŠALIGNã€‹-å¤§åŠ›å‡ºå¥‡è¿¹ï¼Œè°·æ­Œç”¨18äº¿çš„å›¾åƒ-æ–‡æœ¬å¯¹è®­ç»ƒäº†ä¸€ä¸ªè¿™æ ·çš„æ¨¡å‹ã€‚](https://zhuanlan.zhihu.com/p/410499923)  
    [ã€Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervisionã€‘](https://zhuanlan.zhihu.com/p/410499923)

***

## CVPR2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [Less is More-CVPR2021æœ€ä½³å­¦ç”Ÿè®ºæ–‡æå](https://zhuanlan.zhihu.com/p/388824565)  
    [ã€Less is More: CLIPBERT for Video-and-Language Learning via Sparse Samplingã€‘](https://arxiv.org/abs/2102.06183)

- [CVPR2021-RSTNetï¼šè‡ªé€‚åº”Attentionçš„â€œçœ‹å›¾è¯´è¯â€æ¨¡å‹](https://zhuanlan.zhihu.com/p/394793465)  
    [ã€RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Wordsã€‘](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.html)

- [CVPR2021 Oralã€ŠSeeing Out of the Boxã€‹åŒ—ç§‘å¤§&ä¸­å±±å¤§å­¦&å¾®è½¯æå‡ºç«¯åˆ°ç«¯è§†è§‰è¯­è¨€è¡¨å¾é¢„è®­ç»ƒæ–¹æ³•](https://zhuanlan.zhihu.com/p/395982625)   
    [ã€Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learningã€‘](https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Seeing_Out_of_the_Box_End-to-End_Pre-Training_for_Vision-Language_Representation_CVPR_2021_paper.html)

- [CVPR2021-å¼€æ”¾å¼çš„Video Captioningï¼Œä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€æå‡ºåŸºäºâ€œæ£€ç´¢-å¤åˆ¶-ç”Ÿæˆâ€çš„ç½‘ç»œ](https://zhuanlan.zhihu.com/p/401333569)   
    [ã€Open-book Video Captioning with Retrieve-Copy-Generate Networkã€‘](https://arxiv.org/abs/2103.05284)

- [CVPR2021-å¤šæ¨¡æ€ä»»åŠ¡æ–°è¿›å±•ï¼å“¥å¤§&Facebookæå‡ºVX2TEXTæ¨¡å‹ï¼Œå®ç°äº†â€œè§†é¢‘+Xâ€åˆ°â€œæ–‡æœ¬â€çš„ä»»åŠ¡](https://zhuanlan.zhihu.com/p/403340498)   
    [ã€VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputsã€‘](https://arxiv.org/abs/2101.12059)

- [CVPR2021-äººå¤§æå‡ºæ–°æ¨¡å‹ï¼Œå°†Two Stageçš„Video Paragraph Captioningå˜æˆOne Stageï¼Œæ€§èƒ½å´æ²¡ä¸‹é™](https://zhuanlan.zhihu.com/p/404419987)   
    [ã€Towards Diverse Paragraph Captioning for Untrimmed Videosã€‘](https://arxiv.org/abs/2105.14477)


- [CVPR2021-ç”¨æ›´å¥½çš„ç›®æ ‡æ£€æµ‹å™¨æå–è§†è§‰ç‰¹å¾ï¼å¾®è½¯æå‡ºVinVLï¼ŒåŸºäºæ›´å¥½çš„è§†è§‰ç‰¹å¾ï¼Œè¾¾åˆ°æ›´å¼ºçš„å¤šæ¨¡æ€æ€§èƒ½ã€‚](https://zhuanlan.zhihu.com/p/422114283)   
    [ã€VinVL: Revisiting Visual Representations in Vision-Language Modelsã€‘](https://arxiv.org/abs/2104.13682)



- [CVPR2021 Oral-ä¸å†éœ€è¦åå¤„ç†æ­¥éª¤ï¼Kakaoæå‡ºç«¯åˆ°ç«¯çš„Human-Objectäº¤äº’æ£€æµ‹æ¨¡å‹](https://zhuanlan.zhihu.com/p/426929486)   
    [ã€HOTR: End-to-End Human-Object Interaction Detection with Transformersã€‘](https://arxiv.org/abs/2101.00529)


### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [è°·æ­Œæ–°ä½œHaloNetï¼šTransformerä¸€ä½œç”¨Self-Attentionçš„æ–¹å¼è¿›è¡Œå·ç§¯](https://zhuanlan.zhihu.com/p/388598744)  
    [ã€Scaling Local Self-Attention for Parameter Efficient Visual Backbonesã€‘](https://zhuanlan.zhihu.com/p/388598744)

- [Involutionï¼ˆé™„å¯¹Involutionçš„æ€è€ƒï¼‰ï¼šæ¸¯ç§‘å¤§ã€å­—èŠ‚è·³åŠ¨ã€åŒ—å¤§æå‡ºâ€œå†…å·â€ç¥ç»ç½‘ç»œç®—å­ï¼Œåœ¨CVä¸‰å¤§ä»»åŠ¡ä¸Šæç‚¹æ˜æ˜¾](https://zhuanlan.zhihu.com/p/395950242)  
    [ã€Involution: Inverting the Inherence of Convolution for Visual Recognitionã€‘](https://arxiv.org/pdf/2103.06255.pdf)

- [CVPR2021-æ¯”CNNå’ŒTransformeræ›´å¥½çš„Backboneï¼ŸUC Berkeley&Google Research,æå‡ºBoTNetï¼ŒImageNetä¸Šç²¾åº¦è¾¾84.7%](https://zhuanlan.zhihu.com/p/418096136)  
    [ã€Bottleneck Transformers for Visual Recognitionã€‘](https://arxiv.org/abs/2101.11605)

### ç›®æ ‡æ£€æµ‹ï¼ˆDetectionï¼‰


- [CVPR2021 Oral-æ”¶æ•›æ›´å¿«ï¼ç²¾åº¦æ›´é«˜ï¼å—ç§‘å¤§&è…¾è®¯å¾®ä¿¡å›¢é˜Ÿé‡ç£…å¼€æºæ— ç›‘ç£é¢„è®­ç»ƒçš„UP-DETR](https://zhuanlan.zhihu.com/p/419660108)  
    [ã€UP-DETR: Unsupervised Pre-training for Object Detection with Transformersã€‘](https://arxiv.org/abs/2011.09094)


***


## SIGIR 2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [SIGIR 2021 æœ€ä½³å­¦ç”Ÿè®ºæ–‡-å›¾åƒæ–‡æœ¬æ£€ç´¢çš„åŠ¨æ€æ¨¡æ€äº¤äº’å»ºæ¨¡](https://zhuanlan.zhihu.com/p/402122260)  
    [ã€Dynamic Modality Interaction Modeling for Image-Text Retrievalã€‘](https://dl.acm.org/doi/abs/10.1145/3404835.3462829)

    
- [SimVLM-æ‹’ç»å„ç§èŠ±é‡Œèƒ¡å“¨ï¼CMU&Googleæå‡ºå¼±ç›‘ç£æç®€VLPæ¨¡å‹ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæ€§èƒ½SOTA](https://zhuanlan.zhihu.com/p/406354414)  
    [ã€SimVLM: Simple Visual Language Model Pretraining with Weak Supervisionã€‘](https://zhuanlan.zhihu.com/p/406354414)

***



## EMNLP2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [å¤šæ¨¡æ€TransformerçœŸçš„å¤šæ¨¡æ€äº†å—ï¼Ÿè®ºå¤šæ¨¡æ€Transformerå¯¹è·¨æ¨¡æ€çš„å½±å“](https://zhuanlan.zhihu.com/p/411890653)  
    [ã€Vision-and-Language or Vision-for-Language? On Cross-Modal Inflfluence in Multimodal Transformersã€‘](https://arxiv.org/abs/2109.04448)

- [EMNLP2021-â€œTransformer+é¢„è®­ç»ƒâ€å†ä¸‹ä¸€åŸï¼Œæ¸¯ç§‘å¤§å¼€æºé«˜æ•ˆçš„å¤šæ¨¡æ€æ‘˜è¦æ€»ç»“ç½‘ç»œ](https://zhuanlan.zhihu.com/p/418923591)  
    [ã€Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarizationã€‘](https://arxiv.org/abs/2109.02401)

***

## TPAMI
### å‹ç¼©åŠ é€Ÿ
- [TPAMI2021-åä¸ºè¯ºäºš&æ‚‰å°¼å¤§å­¦é™¶å¤§ç¨‹å›¢é˜Ÿæå‡ºå¤šåŠŸèƒ½å·ç§¯ï¼ŒåŠ©åŠ›è½»é‡çº§ç½‘ç»œ](https://zhuanlan.zhihu.com/p/423130563)  
    [ã€Learning Versatile Convolution Filters for Efficient Visual Recognitionã€‘](https://arxiv.org/abs/2109.09310v1)

***

## ArXiv
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [OutLook Attentionï¼šå…·æœ‰å±€éƒ¨ä¿¡æ¯æ„ŸçŸ¥èƒ½åŠ›çš„ViT](https://zhuanlan.zhihu.com/p/385561050)  
    [ã€VOLO: Vision Outlooker for Visual Recognitionã€‘](https://arxiv.org/abs/2106.13112)

- [CoAtNetï¼šå·ç§¯+æ³¨æ„åŠ›=ï¼Ÿï¼Ÿï¼Ÿ](https://zhuanlan.zhihu.com/p/385578588)  
    [ã€CoAtNet: Marrying Convolution and Attention for All Data Sizesã€‘](https://arxiv.org/abs/2106.04803)


- [CSWin-Tï¼šå¾®è½¯ã€ä¸­ç§‘å¤§æå‡ºåå­—å½¢æ³¨æ„åŠ›çš„CSWin Transformer](https://zhuanlan.zhihu.com/p/388370370)  
    [ã€CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windowsã€‘](https://arxiv.org/abs/2107.00652)

- [Circle Kernelï¼šæ¸…åé»„é«˜å›¢é˜Ÿã€åº·å¥ˆå°”å¤§å­¦æå‡ºåœ†å½¢å·ç§¯ï¼Œè¿›ä¸€æ­¥æå‡å·ç§¯ç»“æ„çš„æ€§èƒ½](https://zhuanlan.zhihu.com/p/389159556)  
    [ã€Integrating Circle Kernels into Convolutional Neural Networksã€‘](https://arxiv.org/abs/2107.02451)

- [è§†è§‰è§£æå™¨ViPï¼šç‰›æ´¥å¤§å­¦&å­—èŠ‚è·³åŠ¨æå‡ºVisual Parserï¼Œæ˜¾å¼å»ºæ¨¡é«˜çº§è¯­ä¹‰ä¿¡æ¯](https://zhuanlan.zhihu.com/p/390765725)  
    [ã€Visual Parser: Representing Part-whole Hierarchies with Transformersã€‘](https://arxiv.org/abs/2107.05790)
    
- [LG-Transformerï¼šå…¨å±€å’Œå±€éƒ¨å»ºæ¨¡Transformerç»“æ„æ–°ä½œ](https://zhuanlan.zhihu.com/p/393202842)  
    [ã€Local-to-Global Self-Attention in Vision Transformersã€‘](https://arxiv.org/abs/2107.04735)

- [CoTNet-é‡ç£…å¼€æºï¼äº¬ä¸œAI Researchæå‡ºæ–°çš„ä¸»å¹²ç½‘ç»œCoTNet,åœ¨CVPRä¸Šè·å¾—å¼€æ”¾åŸŸå›¾åƒè¯†åˆ«ç«èµ›å† å†›](https://zhuanlan.zhihu.com/p/394795481)  
    [ã€Contextual Transformer Networks for Visual Recognitionã€‘](https://arxiv.org/abs/2107.12292)

- [SÂ²-MLPv2-ç™¾åº¦æå‡ºç›®å‰æœ€å¼ºçš„è§†è§‰MLPæ¶æ„ï¼Œè¶…è¶ŠMLP-Mixerã€Swin Transformerã€CycleMLPç­‰ï¼Œè¾¾åˆ°83.6% Top-1å‡†ç¡®ç‡](https://zhuanlan.zhihu.com/p/397003638)  
    [ã€SÂ²-MLPv2: Improved Spatial-Shift MLP Architecture for Visionã€‘](https://arxiv.org/abs/2108.01072)

- [æ›´æ·±å’Œæ›´å®½çš„Transformerï¼Œé‚£ä¸ªæ¯”è¾ƒå¥½ï¼ŸNUSå›¢é˜Ÿç»™å‡ºäº†ç»™å‡ºâ€œGo Wider Instead of Deeperâ€çš„ç»“è®º](https://zhuanlan.zhihu.com/p/398168686)  
    [ã€Go Wider Instead of Deeperã€‘](https://arxiv.org/abs/2107.11817)

- [åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šæ€’æ¶¨8.6 APï¼Œå¾®è½¯æ–°ä½œMobileFormer](https://zhuanlan.zhihu.com/p/400291282)  
    [ã€Mobile-Former: Bridging MobileNet and Transformerã€‘](https://arxiv.org/abs/2108.05895)

- [åˆç®€å•åˆå¥½ç”¨çš„Transformerå˜ä½“ï¼æ¸…å&MSRAå¼€æºçº¿æ€§å¤æ‚åº¦çš„Fastformerï¼](https://zhuanlan.zhihu.com/p/409050589)  
    [ã€Fastformer: Additive Attention Can Be All You Needã€‘](https://arxiv.org/abs/2108.09084)

- [ã€ŠVisformerã€‹-å¯¹è§†è§‰ä»»åŠ¡æ›´å‹å¥½çš„Transformerï¼ŒåŒ—èˆªå›¢é˜Ÿå¼€æºVisformerï¼](https://zhuanlan.zhihu.com/p/409784985)  
    [ã€Visformer: The Vision-friendly Transformerã€‘](https://arxiv.org/abs/2104.12533v4)

- [ã€ŠCrossFormerã€‹-ç®€å•é«˜æ•ˆï¼æµ™å¤§CAD&è…¾è®¯&å“¥å¤§å¼€æºè·¨å°ºåº¦çš„Transformerï¼Œæ˜¾è‘—æ¶¨ç‚¹æ£€æµ‹ã€åˆ†å‰²ã€åˆ†ç±»ä¸‰å¤§CVä»»åŠ¡](https://zhuanlan.zhihu.com/p/410155334)  
    [ã€CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attentionã€‘](https://arxiv.org/abs/2108.00154)


- [ä½ è§è¿‡é•¿å¾—åƒCNNçš„MLPå—ï¼ŸUO&UIUCæå‡ºäº†ç”¨äºè§†è§‰ä»»åŠ¡çš„å±‚æ¬¡å·ç§¯MLP](https://zhuanlan.zhihu.com/p/418094475)  
    [ã€ConvMLP: Hierarchical Convolutional MLPs for Visionã€‘](https://arxiv.org/abs/2109.04454)


- [Self-AttentionçœŸçš„æ˜¯å¿…è¦çš„å—ï¼Ÿå¾®è½¯&ä¸­ç§‘å¤§æå‡ºSparse MLPï¼Œé™ä½è®¡ç®—é‡çš„åŒæ—¶æå‡æ€§èƒ½ï¼](https://zhuanlan.zhihu.com/p/418093199)  
    [ã€Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?ã€‘](https://arxiv.org/abs/2109.05422)



- [ç›®æ ‡æ£€æµ‹å†æ¬¡é©æ–°ï¼å›¾çµå¥–å¾—ä¸»Hintonå›¢é˜Ÿæå‡ºPix2Seqï¼Œå°†Detectionå˜æˆäº†Image Captioning](https://zhuanlan.zhihu.com/p/418095279)  
    [ã€Pix2seq: A Language Modeling Framework for Object Detectionã€‘](https://arxiv.org/abs/2109.10852)



- [å®ƒæ¥äº†ï¼è½»é‡ã€é€šç”¨ã€é€‚ç”¨äºç§»åŠ¨è®¾å¤‡çš„Transformerï¼è‹¹æœå…¬å¸æå‡ºäº†MobileViT](https://zhuanlan.zhihu.com/p/424669337)  
    [ã€MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformerã€‘](https://arxiv.org/abs/2110.02178)



- [ã€ŠUFO-ViTã€‹-Transformerå¯ä»¥ä¸éœ€è¦Softmaxï¼ŸKakaoæå‡ºäº†UFO-ViTï¼Œæ€§èƒ½é«˜ï¼Œè®¡ç®—é‡è¿˜å°](https://zhuanlan.zhihu.com/p/431194075)  
    [ã€UFO-ViT: High Performance Linear Vision Transformer without Softmaxã€‘](https://arxiv.org/abs/2109.14382)



### åˆ†å‰²ï¼ˆSegmentationï¼‰
- [MaskFormerï¼šè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²â€œå¤§ä¸€ç»Ÿâ€ï¼šFacebook&UIUCæå‡ºMaskFormer](https://zhuanlan.zhihu.com/p/392731360)  
    [ã€Per-Pixel Classification is Not All You Need for Semantic Segmentationã€‘](https://arxiv.org/abs/2107.06278)

- [æ–°çš„é€šé“å’Œç©ºé—´æ³¨æ„åŠ›å»ºæ¨¡ç»“æ„Polarized Self-Attentionï¼Œéœ¸æ¦œCOCOäººä½“å§¿æ€ä¼°è®¡å’ŒCityscapesè¯­ä¹‰åˆ†å‰²](https://zhuanlan.zhihu.com/p/389770482)  
    [ã€Polarized Self-Attention: Towards High-quality Pixel-wise Regressionã€‘](https://arxiv.org/pdf/2107.00782.pdf)


- [å…¨æ™¯åˆ†å‰²ç¬¬ä¸€åï¼å—å¤§&æ¸¯å¤§&NVIDIAæå‡ºPanoptic SegFormerï¼Œéœ¸æ¦œå…¨æ™¯åˆ†å‰²](https://zhuanlan.zhihu.com/p/418088118)  
    [ã€Panoptic SegFormerã€‘](https://arxiv.org/abs/2109.03814)


- [ä¸­ç§‘é™¢&è¥¿äº¤&æ—·è§†ï¼ˆå­™å‰‘å›¢é˜Ÿï¼‰æå‡ºç”¨äºè¯­ä¹‰åˆ†å‰²çš„åŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œç²¾ç¡®æ„ŸçŸ¥å¤šå°ºåº¦ç›®æ ‡ï¼Œä»£ç å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/427709226)  
    [ã€Learning Dynamic Routing for Semantic Segmentationã€‘](https://arxiv.org/abs/2003.10401)


### æ£€æµ‹ï¼ˆDetectionï¼‰
- [ã€ŠAnchor DETRã€‹-åŠ äº†Anchor Pointèƒ½å¤Ÿè®©DETRåˆå¿«åˆå¥½ï¼Ÿæ—·è§†å­™å‰‘å›¢é˜Ÿæå‡ºAnchor DETR](https://zhuanlan.zhihu.com/p/411889426)  
    [ã€Anchor DETR: Query Design for Transformer-Based Detectorã€‘](https://arxiv.org/abs/2109.07107)



- [åŠ äº†Anchor Pointèƒ½å¤Ÿè®©DETRåˆå¿«åˆå¥½ï¼Ÿæ—·è§†å­™å‰‘å¤§ä½¬å›¢é˜Ÿæå‡ºAnchor DETR](https://zhuanlan.zhihu.com/p/415578473)  
    [ã€Anchor DETR: Query Design for Transformer-Based Detectorã€‘](https://arxiv.org/abs/2109.07107)


### å¢é‡å­¦ä¹ ï¼ˆIncremental Learningï¼‰
- [è®©æ¨¡å‹å®ç°â€œç»ˆç”Ÿå­¦ä¹ â€ï¼Œä½æ²»äºšç†å·¥å­¦é™¢æå‡ºData-Freeçš„å¢é‡å­¦ä¹ ](https://zhuanlan.zhihu.com/p/399085992)  
    [ã€Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learningã€‘](https://arxiv.org/abs/2106.09701)

### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [å›½ç§‘å¤§æå‡ºç”¨äºVideoQAçš„è·¨æ¨¡æ€äº¤äº’æ—¶é—´é‡‘å­—å¡”Transformer](https://zhuanlan.zhihu.com/p/419923517)  
    [ã€Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answeringã€‘](https://arxiv.org/abs/2109.04735)

- [10äº¿å‚æ•°ï¼åˆ«åªç©GPTï¼Œæ¥çœ‹çœ‹è¿™ä¸ªå·²ç»è½åœ°çš„å›½äº§æ¨¡å‹BriVLï¼äººå¤§&ä¸­ç§‘é™¢è”æ‰‹æ‰“é€ ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡æ€ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](https://zhuanlan.zhihu.com/p/425672126)  
    [ã€WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Trainingã€‘](https://arxiv.org/abs/2103.06561)


- [CLIPå¯¹è§†è§‰å’Œè¯­è¨€ä»»åŠ¡æœ‰å¤šå¤§çš„å¥½å¤„ï¼ŸUC Berkeley&UCLAå›¢é˜Ÿç»™å‡ºäº†ç­”æ¡ˆï¼](https://zhuanlan.zhihu.com/p/429243265)  
    [ã€How Much Can CLIP Benefit Vision-and-Language Tasks?ã€‘](https://arxiv.org/abs/2107.06383)


- [æ¶ˆé™¤é¢„è®­ç»ƒæ¨¡å‹çš„è¯­è¨€é™åˆ¶ï¼Googleæå‡ºè·¨è¯­è¨€çš„å¤šæ¨¡æ€ã€å¤šä»»åŠ¡æ£€ç´¢æ¨¡å‹MURAL](https://zhuanlan.zhihu.com/p/418098303)  
    [ã€MURAL: Multimodal, Multitask Retrieval Across Languagesã€‘](https://arxiv.org/abs/2109.05125v1)

### è§†é¢‘ï¼ˆVideoï¼‰
- [Video Swin Transformer-æ—¢Swin Transformerä¹‹åï¼ŒMSRAå¼€æºVideo Swin Transformerï¼Œåœ¨è§†é¢‘æ•°æ®é›†ä¸ŠSOTA](https://zhuanlan.zhihu.com/p/401600421)  
    [ã€Video Swin Transformerã€‘](https://arxiv.org/abs/2106.13230)

- [åŸºäºæ—¶ç©ºæ··åˆattentionçš„è§†é¢‘Transformerï¼Œå¤§å¹…åº¦é™ä½è®¡ç®—å¤æ‚åº¦](https://zhuanlan.zhihu.com/p/420280467)  
    [ã€Space-time Mixing Attention for Video Transformerã€‘](https://arxiv.org/abs/2106.05968)



### å‹ç¼©åŠ é€Ÿ

- [DynamicViT-è¿˜åœ¨ç”¨å…¨éƒ¨tokenè®­ç»ƒViTï¼Ÿæ¸…å&UCLAæå‡ºtokençš„åŠ¨æ€ç¨€ç–åŒ–é‡‡æ ·ï¼Œé™ä½inferenceæ—¶çš„è®¡ç®—é‡](https://zhuanlan.zhihu.com/p/405326718)  
    [ã€DynamicViT: Effificient Vision Transformers with Dynamic Token Sparsifificationã€‘](https://arxiv.org/abs/2106.02034)
    

- [åŠ é€Ÿäº†DeiT-S 60%+çš„ååé‡ï¼è‡ªåŠ¨åŒ–æ‰€&ä¸Šäº¤&ä¼˜å›¾æå‡ºEvo-ViTï¼Œç”¨Slow-Fastçš„æ–¹å¼æ›´æ–°token](https://zhuanlan.zhihu.com/p/412199816)  
    [ã€Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformerã€‘](https://arxiv.org/abs/2108.01390v3)


- [å‹ç¼©ä¹‹åç¥ç»ç½‘ç»œå¿˜è®°äº†ä»€ä¹ˆï¼ŸGoogleç ”ç©¶å‘˜ç»™å‡ºäº†ç­”æ¡ˆ](https://zhuanlan.zhihu.com/p/418099910)  
    [ã€What Do Compressed Deep Neural Networks Forget?ã€‘](https://arxiv.org/abs/1911.05248)



### åŠ¨æ€ç½‘ç»œ

- [æµ™å¤§&åä¸ºè¯ºäºš&è¥¿æ¹–å¤§å­¦æå‡ºç”¨äºç›®æ ‡æ£€æµ‹çš„åŠ¨æ€ç‰¹å¾é‡‘å­—å¡”DyFPNï¼Œå‡å°‘40%çš„FLOPsï¼](https://zhuanlan.zhihu.com/p/428439288)  
    [ã€Dynamic Feature Pyramid Networks for Object Detectionã€‘](https://arxiv.org/abs/2012.00779)

- [ã€ŠDynamic Routingã€‹-ä¸­ç§‘é™¢&è¥¿äº¤&æ—·è§†ï¼ˆå­™å‰‘å›¢é˜Ÿï¼‰æå‡ºç”¨äºè¯­ä¹‰åˆ†å‰²çš„åŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œç²¾ç¡®æ„ŸçŸ¥å¤šå°ºåº¦ç›®æ ‡ï¼Œä»£ç å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/430452628)  
    [ã€Learning Dynamic Routing for Semantic Segmentationã€‘](https://arxiv.org/abs/2003.10401)

- [æ™®æ—æ–¯é¡¿å¤§å­¦&è‹±ä¼Ÿè¾¾&Facebookæå‡ºåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å…¨åŠ¨æ€æ¨ç†ï¼ŒåŠ©åŠ›è½»é‡çº§ç½‘ç»œï¼](https://zhuanlan.zhihu.com/p/430518300)  
    [ã€Fully Dynamic Inference with Deep Neural Networksã€‘](https://arxiv.org/abs/2007.15151)


### å¤šæ¨¡æ€æ£€ç´¢


- [CLIPå†åˆ›è¾‰ç…Œï¼è¥¿å—äº¤å¤§&MSRAæå‡ºCLIP4Clipï¼Œè¿›è¡Œç«¯åˆ°ç«¯çš„è§†é¢‘æ–‡æœ¬æ£€ç´¢ï¼](https://zhuanlan.zhihu.com/p/433063611)  
    [ã€CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrievalã€‘](https://arxiv.org/abs/2104.08860)

- [è…¾è®¯PCGæå‡ºCLIP2Videoï¼ŒåŸºäºCLIPè§£å†³è§†é¢‘æ–‡æœ¬æ£€ç´¢é—®é¢˜ï¼Œæ€§èƒ½SOTAï¼ä»£ç å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/433083355)  
    [ã€CLIP2Video: Mastering Video-Text Retrieval via Image CLIPã€‘](https://arxiv.org/abs/2106.11097)


### å…¶ä»–


- [æ‹’ç»Prompt Engineeringï¼ŒNTUæå‡ºCoOpï¼Œè‡ªé€‚åº”å­¦ä¹ ä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„Promptï¼Œæ€§èƒ½ç¢¾å‹æ‰‹å·¥è®¾è®¡çš„Prompt](https://zhuanlan.zhihu.com/p/408190719)  
    [ã€Learning to Prompt for Vision-Language Modelsã€‘](https://arxiv.org/abs/2109.01134)



- [æ·±åº¦ç¥ç»ç½‘ç»œå…¶å®å¹¶ä¸éœ€è¦é‚£ä¹ˆæ·±ï¼æ™®æ—æ–¯é¡¿å¤§å­¦&Intelæå‡ºParNetï¼Œ12å±‚çš„ç½‘ç»œå°±èƒ½è¾¾åˆ°80%ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼](https://zhuanlan.zhihu.com/p/429732072)  
    [ã€Non-deep Networksã€‘](https://arxiv.org/abs/2110.07641)




- [NeurIPS2021-æ¸¯å¤§&è…¾è®¯AI Lab&ç‰›æ´¥å¤§å­¦æå‡ºCAREï¼Œè®©CNNå’ŒTransformerèƒ½åœ¨å¯¹æ¯”å­¦ä¹ ä¸­â€œäº’å¸®äº’åŠ©â€ï¼](https://zhuanlan.zhihu.com/p/430773996)  
    [ã€Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learningã€‘](https://arxiv.org/abs/2110.05340)



- [FAIRä¸‰ç¥Kaimingï¼ŒPiotrï¼ŒRossæ–°ä½œï¼ŒMAEæ‰æ˜¯YYDSï¼ä»…ç”¨ImageNet1Kï¼ŒTop-1å‡†ç¡®ç‡87.8%ï¼Œå°ç¥ï¼](https://zhuanlan.zhihu.com/p/432663453)  
    [ã€Masked Autoencoders Are Scalable Vision Learnersã€‘](https://arxiv.org/abs/2111.06377)


- []()  
    [ã€ã€‘]()