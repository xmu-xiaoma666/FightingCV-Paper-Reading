<img src="./tmpimg/LOGO.gif" height="200" width="400"/>


# FightingCV-Paper-Reading


Helloï¼Œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯å°é©¬ğŸš€ğŸš€ğŸš€

ä½œä¸ºç ”ç©¶ç”Ÿï¼Œè¯»è®ºæ–‡ä¸€ç›´éƒ½æ˜¯éƒ½æ˜¯ä¸€ä»¶éå¸¸**è´¹æ—¶è´¹è„‘**çš„äº‹æƒ…ï¼Œå› ä¸ºå¸®åŠ©å¤§å®¶ç”¨**5åˆ†é’Ÿ**çš„æ—¶é—´å°±èƒ½çŸ¥é“æŸç¯‡è®ºæ–‡çš„å¤§è‡´å†…å®¹ï¼Œæˆ‘ä¼šæŠŠæˆ‘çœ‹è¿‡çš„è®ºæ–‡åšå¥½è§£æåˆ†äº«åœ¨è¿™é‡Œã€‚**é¡¹ç›®æŒç»­æ›´æ–°ï¼Œæ¯å‘¨è‡³å°‘æ›´æ–°ä¸‰ç¯‡ï¼**â­â­â­

æœ¬é¡¹ç›®çš„å®—æ—¨æ˜¯ğŸš€**è®©ä¸–ç•Œä¸Šæ²¡æœ‰éš¾è¯»çš„è®ºæ–‡**ğŸš€ï¼Œè®ºæ–‡ä¸»é¢˜åŒ…æ‹¬ä½†ä¸é™äºæ£€æµ‹ã€åˆ†ç±»ã€åˆ†å‰²ã€Backboneã€å¤šæ¨¡æ€ç­‰ç­‰ï¼Œè®ºæ–‡æ¥æºåŒ…æ‹¬ä½†ä¸é™äºæœ€æ–°çš„arXivè®ºæ–‡ã€ICCV2021ã€CVPR2021ã€MM2021ã€‚**(é¡¹ç›®ä¼šä¿æŒæŒç»­æ›´æ–°ï¼Œæ¯å‘¨è‡³å°‘ä¸‰ç¯‡)**â­â­â­
 

ï¼ˆæœ€æ–°è¿˜æ›´æ–°äº†[ã€Attentionã€MLPã€Convã€MLPã€Backboneçš„ä»£ç å¤ç°é¡¹ç›®ã€‘](https://github.com/xmu-xiaoma666/External-Attention-pytorch)ï¼Œæ¬¢è¿å¤§å®¶å­¦ä¹ äº¤æµï¼‰


***
## å…¬ä¼—å· & å¾®ä¿¡äº¤æµç¾¤

æ¬¢è¿å¤§å®¶å…³æ³¨å…¬ä¼—å·ï¼š**FightingCV**

å…¬ä¼—å·**æ¯å¤©**éƒ½ä¼šè¿›è¡Œ**è®ºæ–‡ã€ç®—æ³•å’Œä»£ç çš„å¹²è´§åˆ†äº«**å“¦~


å·²å»ºç«‹**æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç®—æ³•/è®¡ç®—æœºè§†è§‰/å¤šæ¨¡æ€äº¤æµç¾¤**å¾®ä¿¡äº¤æµç¾¤ï¼

**æ¯å¤©åœ¨ç¾¤é‡Œåˆ†äº«ä¸€äº›è¿‘æœŸçš„è®ºæ–‡å’Œè§£æ**ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·**å­¦ä¹ äº¤æµ**å“ˆ~~~

![](./tmpimg/wechat.jpg)

å¼ºçƒˆæ¨èå¤§å®¶å…³æ³¨[**çŸ¥ä¹**](https://www.zhihu.com/people/jason-14-58-38/posts)è´¦å·å’Œ[**FightingCVå…¬ä¼—å·**](https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w)ï¼Œå¯ä»¥å¿«é€Ÿäº†è§£åˆ°æœ€æ–°ä¼˜è´¨çš„å¹²è´§èµ„æºã€‚




## CVPR2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [Less is More-CVPR2021æœ€ä½³å­¦ç”Ÿè®ºæ–‡æå](https://zhuanlan.zhihu.com/p/388824565)  
    [ã€Less is More: CLIPBERT for Video-and-Language Learning via Sparse Samplingã€‘](https://arxiv.org/abs/2102.06183)
- [CVPR2021-RSTNetï¼šè‡ªé€‚åº”Attentionçš„â€œçœ‹å›¾è¯´è¯â€æ¨¡å‹](https://zhuanlan.zhihu.com/p/394793465)  
    [ã€RSTNet: Captioning With Adaptive Attention on Visual and Non-Visual Wordsã€‘](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.html)
- [CVPR2021 Oralã€ŠSeeing Out of the Boxã€‹åŒ—ç§‘å¤§&ä¸­å±±å¤§å­¦&å¾®è½¯æå‡ºç«¯åˆ°ç«¯è§†è§‰è¯­è¨€è¡¨å¾é¢„è®­ç»ƒæ–¹æ³•](https://zhuanlan.zhihu.com/p/395982625)   
    [ã€Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learningã€‘](https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Seeing_Out_of_the_Box_End-to-End_Pre-Training_for_Vision-Language_Representation_CVPR_2021_paper.html)
- [CVPR2021-å¼€æ”¾å¼çš„Video Captioningï¼Œä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€æå‡ºåŸºäºâ€œæ£€ç´¢-å¤åˆ¶-ç”Ÿæˆâ€çš„ç½‘ç»œ](https://zhuanlan.zhihu.com/p/401333569)   
    [ã€Open-book Video Captioning with Retrieve-Copy-Generate Networkã€‘](https://arxiv.org/abs/2103.05284)
- [CVPR2021-å¤šæ¨¡æ€ä»»åŠ¡æ–°è¿›å±•ï¼å“¥å¤§&Facebookæå‡ºVX2TEXTæ¨¡å‹ï¼Œå®ç°äº†â€œè§†é¢‘+Xâ€åˆ°â€œæ–‡æœ¬â€çš„ä»»åŠ¡](https://zhuanlan.zhihu.com/p/403340498)   
    [ã€VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputsã€‘](https://arxiv.org/abs/2101.12059)
- [CVPR2021-äººå¤§æå‡ºæ–°æ¨¡å‹ï¼Œå°†Two Stageçš„Video Paragraph Captioningå˜æˆOne Stageï¼Œæ€§èƒ½å´æ²¡ä¸‹é™](https://zhuanlan.zhihu.com/p/404419987)   
    [ã€Towards Diverse Paragraph Captioning for Untrimmed Videosã€‘](https://arxiv.org/abs/2105.14477)
- []()
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [è°·æ­Œæ–°ä½œHaloNetï¼šTransformerä¸€ä½œç”¨Self-Attentionçš„æ–¹å¼è¿›è¡Œå·ç§¯](https://zhuanlan.zhihu.com/p/388598744)  
    [ã€Scaling Local Self-Attention for Parameter Efficient Visual Backbonesã€‘](https://zhuanlan.zhihu.com/p/388598744)
- [Involutionï¼ˆé™„å¯¹Involutionçš„æ€è€ƒï¼‰ï¼šæ¸¯ç§‘å¤§ã€å­—èŠ‚è·³åŠ¨ã€åŒ—å¤§æå‡ºâ€œå†…å·â€ç¥ç»ç½‘ç»œç®—å­ï¼Œåœ¨CVä¸‰å¤§ä»»åŠ¡ä¸Šæç‚¹æ˜æ˜¾](https://zhuanlan.zhihu.com/p/395950242)  
    [ã€Involution: Inverting the Inherence of Convolution for Visual Recognitionã€‘](https://arxiv.org/pdf/2103.06255.pdf)
- []()



## ICCV2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [ICCV2021 Oral-MDETRï¼šå›¾çµå¥–å¾—ä¸»Yann LeCunçš„å›¢é˜Ÿ&Facebookæå‡ºç«¯åˆ°ç«¯å¤šæ¨¡æ€ç†è§£çš„ç›®æ ‡æ£€æµ‹å™¨](https://zhuanlan.zhihu.com/p/394239659)  
    [ã€MDETR -- Modulated Detection for End-to-End Multi-Modal Understandingã€‘](https://arxiv.org/abs/2104.12763)
- [ICCV2021-NTUç”¨å¤šæ ·æ€§çš„queryç”Ÿæˆï¼Œæ¶¨ç‚¹åŸºäºæ–‡æœ¬çš„å®ä¾‹åˆ†å‰²ï¼ˆå·²å¼€æºï¼‰](https://zhuanlan.zhihu.com/p/404955179)  
    [ã€Vision-Language Transformer and Query Generation for Referring Segmentationã€‘](https://arxiv.org/abs/2108.05565)

### å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰
- [ICCV2021-DetCoï¼šæ€§èƒ½ä¼˜äºä½•æºæ˜ç­‰äººæå‡ºçš„MoCo v2ï¼Œä¸ºç›®æ ‡æ£€æµ‹å®šåˆ¶ä»»åŠ¡çš„å¯¹æ¯”å­¦ä¹ ã€‚](https://zhuanlan.zhihu.com/p/393202411)  
    [ã€DetCo: Unsupervised Contrastive Learning for Object Detectionã€‘](https://arxiv.org/abs/2102.04803)
- []()
### å¯è§£é‡Šæ€§ï¼ˆInterpretabilityï¼‰
- [ICCV2021 Oral-TAU&Facebookæå‡ºäº†é€šç”¨çš„Attentionæ¨¡å‹å¯è§£é‡Šæ€§](https://zhuanlan.zhihu.com/p/394794493)  
    [ã€Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformersã€‘](https://arxiv.org/abs/2103.15679)
- [ICCV2021 -ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿåˆ†ç±»æ­£ç¡®ï¼ŸSCOUTERèƒ½å¤Ÿâ€œæ­£â€â€œåâ€ä¸¤ä¸ªæ–¹é¢è¯´æœä½ ã€‚](https://zhuanlan.zhihu.com/p/396783525)  
    [ã€SCOUTER: Slot Attention-based Classifier for Explainable Image Recognitionã€‘](https://arxiv.org/abs/2009.06138)
- []()
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [ICCV2021-iRPE-è¿˜åœ¨é­”æ”¹Transformerç»“æ„å—ï¼Ÿå¾®è½¯&ä¸­å±±å¤§å­¦æå‡ºè¶…å¼ºçš„å›¾ç‰‡ä½ç½®ç¼–ç ï¼Œæ¶¨ç‚¹æ˜¾è‘—](https://zhuanlan.zhihu.com/p/395766591)   
    [ã€Rethinking and Improving Relative Position Encoding for Vision Transformerã€‘](https://arxiv.org/abs/2107.14222)
- [ICCV2021 | æ± åŒ–æ“ä½œä¸æ˜¯CNNçš„ä¸“å±ï¼ŒVision Transformerè¯´ï¼šâ€œæˆ‘ä¹Ÿå¯ä»¥â€ï¼›å—å¤§æå‡ºæ± åŒ–è§†è§‰Transformerï¼ˆPiTï¼‰](https://zhuanlan.zhihu.com/p/398763751)  
    [ã€Rethinking Spatial Dimensions of Vision Transformersã€‘](https://arxiv.org/abs/2103.16302)
- [ICCV2021 | CNN+Transformer=Betterï¼Œå›½ç§‘å¤§&åä¸º&é¹åŸå®éªŒå®¤ å‡ºConformerï¼Œ84.1% Top-1å‡†ç¡®ç‡](https://zhuanlan.zhihu.com/p/400244375)  
    [ã€Conformer: Local Features Coupling Global Representations for Visual Recognitionã€‘](https://arxiv.org/abs/2105.03889)
- [ICCV2021 | MicroNets-æ›´å°æ›´å¿«æ›´å¥½çš„MicroNetï¼Œä¸‰å¤§CVä»»åŠ¡éƒ½ç§’æ€MobileNetV3](https://zhuanlan.zhihu.com/p/400661708)  
    [ã€MicroNet: Improving Image Recognition with Extremely Low FLOPsã€‘](https://arxiv.org/abs/2108.05894)
### å¤šä»»åŠ¡ï¼ˆMulti-Taskï¼‰
- [ICCV2021-MuST-è¿˜åœ¨ç‰¹å®šä»»åŠ¡é‡Œä¸ºåˆ·ç‚¹è€Œè‹¦è‹¦æŒ£æ‰ï¼Ÿè°·æ­Œçš„å¤§ä½¬ä»¬éƒ½å·²ç»å¼€å§‹ç©å¤šä»»åŠ¡è®­ç»ƒäº†](https://zhuanlan.zhihu.com/p/406014791)  
    [ã€Multi-Task Self-Training for Learning General Representationsã€‘](https://arxiv.org/abs/2108.11353)
- [ICCV2021-CVå¤šä»»åŠ¡æ–°è¿›å±•ï¼ä¸€èŠ‚æ›´æ¯”ä¸‰èŠ‚å¼ºçš„MultiTask CenterNetï¼Œç”¨ä¸€ä¸ªç½‘ç»œåŒæ—¶å®Œæˆç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œäººä½“å§¿æ€ä¼°è®¡ä¸‰ä¸ªä»»åŠ¡](https://zhuanlan.zhihu.com/p/405652732)  
    [ã€MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approachã€‘](https://arxiv.org/abs/2108.05060)
### å…¶ä»–
- [ICCV 2021ï½œâ€œç™½å«–â€æ€§èƒ½çš„MixMoï¼Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºoræ¨¡å‹èåˆæ–¹æ³•](https://zhuanlan.zhihu.com/p/396528978)   
    [ã€MicroNet: Improving Image Recognition with Extremely Low FLOPsã€‘](https://arxiv.org/abs/2108.05894)
- [ICCV'21 Oralï½œæ‹’ç»è°ƒå‚ï¼Œæ˜¾è‘—æç‚¹ï¼æ£€æµ‹åˆ†å‰²ä»»åŠ¡çš„æ–°æŸå¤±å‡½æ•°RS Losså¼€æº](https://zhuanlan.zhihu.com/p/397519850)  
    [ã€Rank & Sort Loss for Object Detection and Instance Segmentationã€‘](https://arxiv.org/abs/2107.11669)
- [ICCV21 | å¤§é“è‡³ç®€ï¼Œä»…éœ€4è¡Œä»£ç æå‡å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½ï¼ å—å¤§æå‡ºResidual Attention](https://zhuanlan.zhihu.com/p/397990353)  
    [ã€Residual Attention: A Simple but Effective Method for Multi-Label Recognitionã€‘](https://arxiv.org/abs/2108.02456)
- []()

## ACM MM2021
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [ACM MM2021-è¿˜åœ¨ç”¨ViTçš„16x16 Patchåˆ†å‰²æ–¹æ³•å—ï¼Ÿä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€æå‡ºDeformable Patch-basedæ–¹æ³•ï¼Œæ¶¨ç‚¹æ˜¾è‘—ï¼](https://zhuanlan.zhihu.com/p/399417704)  
    [ã€DPT: Deformable Patch-based Transformer for Visual Recognitionã€‘](https://arxiv.org/abs/2107.14467)
- [ACMMM 2021-å¤šæ¨¡æ€å®è—ï¼äº¬ä¸œæ¢…æ¶›å›¢é˜Ÿé‡ç£…å¼€æºç¬¬ä¸€ä¸ªé€‚ç”¨äºå¤šä¸ªä»»åŠ¡çš„å¤šæ¨¡æ€ä»£ç åº“x-modalerï¼](https://zhuanlan.zhihu.com/p/403688076)  
    [ã€X-modaler: A Versatile and High-performance Codebase for Cross-modal Analyticsã€‘](https://arxiv.org/abs/2108.08217)
- [ACMMM 2021-æ€§èƒ½SOTAï¼ç”¨GNNå’ŒGANçš„æ–¹å¼æ¥å¼ºåŒ–Video Captioningçš„å­¦ä¹ ï¼](https://zhuanlan.zhihu.com/p/403895573)  
    [ã€Discriminative Latent Semantic Graph for Video Captioningã€‘](https://arxiv.org/abs/2108.03662)
- []()

## SIGIR 2021
### å¤šæ¨¡æ€ï¼ˆMulti-Modalï¼‰
- [SIGIR 2021 æœ€ä½³å­¦ç”Ÿè®ºæ–‡-å›¾åƒæ–‡æœ¬æ£€ç´¢çš„åŠ¨æ€æ¨¡æ€äº¤äº’å»ºæ¨¡](https://zhuanlan.zhihu.com/p/402122260)  
    [ã€Dynamic Modality Interaction Modeling for Image-Text Retrievalã€‘](https://dl.acm.org/doi/abs/10.1145/3404835.3462829)
- []()

## ArXiv
### ä¸»å¹²ç½‘ç»œï¼ˆBackboneï¼ŒCNNï¼ŒTransformerï¼‰
- [OutLook Attentionï¼šå…·æœ‰å±€éƒ¨ä¿¡æ¯æ„ŸçŸ¥èƒ½åŠ›çš„ViT](https://zhuanlan.zhihu.com/p/385561050)  
    [ã€VOLO: Vision Outlooker for Visual Recognitionã€‘](https://arxiv.org/abs/2106.13112)
- [CoAtNetï¼šå·ç§¯+æ³¨æ„åŠ›=ï¼Ÿï¼Ÿï¼Ÿ](https://zhuanlan.zhihu.com/p/385578588)  
    [ã€CoAtNet: Marrying Convolution and Attention for All Data Sizesã€‘](https://arxiv.org/abs/2106.04803)
- [Multi-Scale Densenetç»­ä½œï¼ŸåŠ¨æ€ViT](https://zhuanlan.zhihu.com/p/386929227)  
    [ã€Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Lengthã€‘](https://arxiv.org/abs/2105.15075)
- [å¾®è½¯æ–°ä½œFocal Self-Attentionï¼šå…·å¤‡Localå’ŒGlobaläº¤äº’èƒ½åŠ›çš„Transformer](https://zhuanlan.zhihu.com/p/387693270)  
    [ã€Focal Self-attention for Local-Global Interactions in Vision Transformersã€‘](https://arxiv.org/abs/2107.00641)
- [CSWin-Tï¼šå¾®è½¯ã€ä¸­ç§‘å¤§æå‡ºåå­—å½¢æ³¨æ„åŠ›çš„CSWin Transformer](https://zhuanlan.zhihu.com/p/388370370)  
    [ã€CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windowsã€‘](https://arxiv.org/abs/2107.00652)
- [Circle Kernelï¼šæ¸…åé»„é«˜å›¢é˜Ÿã€åº·å¥ˆå°”å¤§å­¦æå‡ºåœ†å½¢å·ç§¯ï¼Œè¿›ä¸€æ­¥æå‡å·ç§¯ç»“æ„çš„æ€§èƒ½](https://zhuanlan.zhihu.com/p/389159556)  
    [ã€Integrating Circle Kernels into Convolutional Neural Networksã€‘](https://arxiv.org/abs/2107.02451)
- [è§†è§‰è§£æå™¨ViPï¼šç‰›æ´¥å¤§å­¦&å­—èŠ‚è·³åŠ¨æå‡ºVisual Parserï¼Œæ˜¾å¼å»ºæ¨¡é«˜çº§è¯­ä¹‰ä¿¡æ¯](https://zhuanlan.zhihu.com/p/390765725)  
    [ã€Visual Parser: Representing Part-whole Hierarchies with Transformersã€‘](https://arxiv.org/abs/2107.05790)
- [LG-Transformerï¼šå…¨å±€å’Œå±€éƒ¨å»ºæ¨¡Transformerç»“æ„æ–°ä½œ](https://zhuanlan.zhihu.com/p/393202842)  
    [ã€Local-to-Global Self-Attention in Vision Transformersã€‘](https://arxiv.org/abs/2107.04735)
- [CoTNet-é‡ç£…å¼€æºï¼äº¬ä¸œAI Researchæå‡ºæ–°çš„ä¸»å¹²ç½‘ç»œCoTNet,åœ¨CVPRä¸Šè·å¾—å¼€æ”¾åŸŸå›¾åƒè¯†åˆ«ç«èµ›å† å†›](https://zhuanlan.zhihu.com/p/394795481)  
    [ã€Contextual Transformer Networks for Visual Recognitionã€‘](https://arxiv.org/abs/2107.12292)
- [SÂ²-MLPv2-ç™¾åº¦æå‡ºç›®å‰æœ€å¼ºçš„è§†è§‰MLPæ¶æ„ï¼Œè¶…è¶ŠMLP-Mixerã€Swin Transformerã€CycleMLPç­‰ï¼Œè¾¾åˆ°83.6% Top-1å‡†ç¡®ç‡](https://zhuanlan.zhihu.com/p/397003638)  
    [ã€SÂ²-MLPv2: Improved Spatial-Shift MLP Architecture for Visionã€‘](https://arxiv.org/abs/2108.01072)
- [æ›´æ·±å’Œæ›´å®½çš„Transformerï¼Œé‚£ä¸ªæ¯”è¾ƒå¥½ï¼ŸNUSå›¢é˜Ÿç»™å‡ºäº†ç»™å‡ºâ€œGo Wider Instead of Deeperâ€çš„ç»“è®º](https://zhuanlan.zhihu.com/p/398168686)  
    [ã€Go Wider Instead of Deeperã€‘](https://arxiv.org/abs/2107.11817)
- [åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šæ€’æ¶¨8.6 APï¼Œå¾®è½¯æ–°ä½œMobileFormer](https://zhuanlan.zhihu.com/p/400291282)  
    [ã€Mobile-Former: Bridging MobileNet and Transformerã€‘](https://arxiv.org/abs/2108.05895)
- []()
### è¯­ä¹‰åˆ†å‰²&å®ä¾‹åˆ†å‰²ï¼ˆSegmentationï¼‰
- [MaskFormerï¼šè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²â€œå¤§ä¸€ç»Ÿâ€ï¼šFacebook&UIUCæå‡ºMaskFormer](https://zhuanlan.zhihu.com/p/392731360)  
    [ã€Per-Pixel Classification is Not All You Need for Semantic Segmentationã€‘](https://arxiv.org/abs/2107.06278)
- [æ–°çš„é€šé“å’Œç©ºé—´æ³¨æ„åŠ›å»ºæ¨¡ç»“æ„Polarized Self-Attentionï¼Œéœ¸æ¦œCOCOäººä½“å§¿æ€ä¼°è®¡å’ŒCityscapesè¯­ä¹‰åˆ†å‰²](https://zhuanlan.zhihu.com/p/389770482)  
    [ã€Polarized Self-Attention: Towards High-quality Pixel-wise Regressionã€‘](https://arxiv.org/pdf/2107.00782.pdf)
- []()
### å¢é‡å­¦ä¹ ï¼ˆIncremental Learningï¼‰
- [è®©æ¨¡å‹å®ç°â€œç»ˆç”Ÿå­¦ä¹ â€ï¼Œä½æ²»äºšç†å·¥å­¦é™¢æå‡ºData-Freeçš„å¢é‡å­¦ä¹ ](https://zhuanlan.zhihu.com/p/399085992)  
    [ã€Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learningã€‘](https://arxiv.org/abs/2106.09701)
- []()
### å…¶ä»–
- [Video Swin Transformer-æ—¢Swin Transformerä¹‹åï¼ŒMSRAå¼€æºVideo Swin Transformerï¼Œåœ¨è§†é¢‘æ•°æ®é›†ä¸ŠSOTA](https://zhuanlan.zhihu.com/p/401600421)  
    [ã€Video Swin Transformerã€‘](https://arxiv.org/abs/2106.13230)
- [DynamicViT-è¿˜åœ¨ç”¨å…¨éƒ¨tokenè®­ç»ƒViTï¼Ÿæ¸…å&UCLAæå‡ºtokençš„åŠ¨æ€ç¨€ç–åŒ–é‡‡æ ·ï¼Œé™ä½inferenceæ—¶çš„è®¡ç®—é‡](https://zhuanlan.zhihu.com/p/405326718)  
    [ã€DynamicViT: Effificient Vision Transformers with Dynamic Token Sparsifificationã€‘](https://arxiv.org/abs/2106.02034)
- []()


