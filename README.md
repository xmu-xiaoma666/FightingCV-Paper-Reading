# FightingCV-Paper-Reading


Hello，大家好，我是小马🚀🚀🚀

作为研究生，读论文一直都是都是一件非常**费时费脑**的事情，因为帮助大家用**5分钟**的时间就能知道某篇论文的大致内容，我会把我看过的论文做好解析分享在这里。**项目持续更新，每周至少更新三篇！**⭐⭐⭐

本项目的宗旨是🚀**让世界上没有难读的论文**🚀，论文主题包括但不限于检测、分类、分割、Backbone、多模态等等，论文来源包括但不限于最新的arXiv论文、ICCV2021、CVPR2021、MM2021。**(项目会保持持续更新，每周至少三篇)**⭐⭐⭐
 

（最新还更新了[【Attention、MLP、Conv、MLP、Backbone的代码复现项目】](https://github.com/xmu-xiaoma666/External-Attention-pytorch)，欢迎大家学习交流）


***
## 公众号 & 微信交流群

欢迎大家关注公众号：**FightingCV**

公众号**每天**都会进行**论文、算法和代码的干货分享**哦~


已建立**机器学习/深度学习算法/计算机视觉/多模态交流群**微信交流群！

**每天在群里分享一些近期的论文和解析**，欢迎大家一起**学习交流**哈~~~

![](./tmpimg/wechat.jpg)

强烈推荐大家关注[**知乎**](https://www.zhihu.com/people/jason-14-58-38/posts)账号和[**FightingCV公众号**](https://mp.weixin.qq.com/s/sgNw6XFBPcD20Ef3ddfE1w)，可以快速了解到最新优质的干货资源。




## CVPR2021
### 多模态（Multi-Modal）
- [Less is More-CVPR2021最佳学生论文提名](https://zhuanlan.zhihu.com/p/388824565) [【Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling】](https://arxiv.org/abs/2102.06183)
- [CVPR2021-RSTNet：自适应Attention的“看图说话”模型](https://zhuanlan.zhihu.com/p/394793465)
- [CVPR2021 Oral《Seeing Out of the Box》北科大&中山大学&微软提出端到端视觉语言表征预训练方法](https://zhuanlan.zhihu.com/p/395982625) [【视频讲解】](https://www.bilibili.com/video/BV19q4y1D7Bu)
- [CVPR2021-开放式的Video Captioning，中科院自动化所提出基于“检索-复制-生成”的网络](https://zhuanlan.zhihu.com/p/401333569) 
- [CVPR2021-多模态任务新进展！哥大&Facebook提出VX2TEXT模型，实现了“视频+X”到“文本”的任务](https://zhuanlan.zhihu.com/p/403340498) 
- [CVPR2021-人大提出新模型，将Two Stage的Video Paragraph Captioning变成One Stage，性能却没下降](https://zhuanlan.zhihu.com/p/404419987) 
- []()
### 主干网络（Backbone，CNN，Transformer）
- [谷歌新作HaloNet：Transformer一作用Self-Attention的方式进行卷积](https://zhuanlan.zhihu.com/p/388598744)
- [Involution（附对Involution的思考）：港科大、字节跳动、北大提出“内卷”神经网络算子，在CV三大任务上提点明显](https://zhuanlan.zhihu.com/p/395950242)
- []()



## ICCV2021
### 多模态（Multi-Modal）
- [ICCV2021 Oral-MDETR：图灵奖得主Yann LeCun的团队&Facebook提出端到端多模态理解的目标检测器](https://zhuanlan.zhihu.com/p/394239659)
- []()
### 对比学习（Contrastive Learning）
- [ICCV2021-DetCo：性能优于何恺明等人提出的MoCo v2，为目标检测定制任务的对比学习。](https://zhuanlan.zhihu.com/p/393202411)
- []()
### 可解释性（Interpretability）
- [ICCV2021 Oral-TAU&Facebook提出了通用的Attention模型可解释性](https://zhuanlan.zhihu.com/p/394794493)
- [ICCV2021 -为什么深度学习模型能够分类正确？SCOUTER能够“正”“反”两个方面说服你。](https://zhuanlan.zhihu.com/p/396783525)
- []()
### 主干网络（Backbone，CNN，Transformer）
- [ICCV2021-iRPE-还在魔改Transformer结构吗？微软&中山大学提出超强的图片位置编码，涨点显著](https://zhuanlan.zhihu.com/p/395766591) [【视频讲解】](https://www.bilibili.com/video/BV19q4y1D7Bu)
- [ICCV2021 | 池化操作不是CNN的专属，Vision Transformer说：“我也可以”；南大提出池化视觉Transformer（PiT）](https://zhuanlan.zhihu.com/p/398763751)
- [ICCV2021 | CNN+Transformer=Better，国科大&华为&鹏城实验室 出Conformer，84.1% Top-1准确率](https://zhuanlan.zhihu.com/p/400244375)
- [ICCV2021 | MicroNets-更小更快更好的MicroNet，三大CV任务都秒杀MobileNetV3](https://zhuanlan.zhihu.com/p/400661708)
- []()
### 其他
- [ICCV 2021｜“白嫖”性能的MixMo，一种新的数据增强or模型融合方法](https://zhuanlan.zhihu.com/p/396528978) [【视频讲解】](https://www.bilibili.com/video/BV19q4y1D7Bu)
- [ICCV'21 Oral｜拒绝调参，显著提点！检测分割任务的新损失函数RS Loss开源](https://zhuanlan.zhihu.com/p/397519850)
- [ICCV21 | 大道至简，仅需4行代码提升多标签分类性能！ 南大提出Residual Attention](https://zhuanlan.zhihu.com/p/397990353)
- []()

## ACM MM2021
### 主干网络（Backbone，CNN，Transformer）
- [ACM MM2021-还在用ViT的16x16 Patch分割方法吗？中科院自动化所提出Deformable Patch-based方法，涨点显著！](https://zhuanlan.zhihu.com/p/399417704)
- [ACMMM 2021-多模态宝藏！京东梅涛团队重磅开源第一个适用于多个任务的多模态代码库x-modaler！](https://zhuanlan.zhihu.com/p/403688076)
- [ACMMM 2021-性能SOTA！用GNN和GAN的方式来强化Video Captioning的学习！](https://zhuanlan.zhihu.com/p/403895573)
- []()

## SIGIR 2021
### 多模态（Multi-Modal）
- [SIGIR 2021 最佳学生论文-图像文本检索的动态模态交互建模](https://zhuanlan.zhihu.com/p/402122260)
- []()

## ArXiv
### 主干网络（Backbone，CNN，Transformer）
- [OutLook Attention：具有局部信息感知能力的ViT](https://zhuanlan.zhihu.com/p/385561050)
- [CoAtNet：卷积+注意力=？？？](https://zhuanlan.zhihu.com/p/385578588)
- [Multi-Scale Densenet续作？动态ViT](https://zhuanlan.zhihu.com/p/386929227)
- [微软新作Focal Self-Attention：具备Local和Global交互能力的Transformer](https://zhuanlan.zhihu.com/p/387693270)
- [CSWin-T：微软、中科大提出十字形注意力的CSWin Transformer](https://zhuanlan.zhihu.com/p/388370370)
- [Circle Kernel：清华黄高团队、康奈尔大学提出圆形卷积，进一步提升卷积结构的性能](https://zhuanlan.zhihu.com/p/389159556)
- [视觉解析器ViP：牛津大学&字节跳动提出Visual Parser，显式建模高级语义信息](https://zhuanlan.zhihu.com/p/390765725)
- [LG-Transformer：全局和局部建模Transformer结构新作](https://zhuanlan.zhihu.com/p/393202842)
- [CoTNet-重磅开源！京东AI Research提出新的主干网络CoTNet,在CVPR上获得开放域图像识别竞赛冠军](https://zhuanlan.zhihu.com/p/394795481)
- [S2-MLPV2-百度提出目前最强的视觉MLP架构，超越MLP-Mixer、Swin Transformer、CycleMLP等，达到83.6% Top-1准确率](https://zhuanlan.zhihu.com/p/397003638)
- [更深和更宽的Transformer，那个比较好？NUS团队给出了给出“Go Wider Instead of Deeper”的结论](https://zhuanlan.zhihu.com/p/398168686)
- [在目标检测任务上怒涨8.6 AP，微软新作MobileFormer](https://zhuanlan.zhihu.com/p/400291282)
- []()
### 语义分割&实例分割（Segmentation）
- [MaskFormer：语义分割、实例分割“大一统”：Facebook&UIUC提出MaskFormer](https://zhuanlan.zhihu.com/p/392731360)
- [新的通道和空间注意力建模结构Polarized Self-Attention，霸榜COCO人体姿态估计和Cityscapes语义分割](https://zhuanlan.zhihu.com/p/389770482)
- []()
### 增量学习（Incremental Learning）
- [让模型实现“终生学习”，佐治亚理工学院提出Data-Free的增量学习](https://zhuanlan.zhihu.com/p/399085992)
- []()
### 其他
- [Video Swin Transformer-既Swin Transformer之后，MSRA开源Video Swin Transformer，在视频数据集上SOTA](https://zhuanlan.zhihu.com/p/401600421)
- []()


